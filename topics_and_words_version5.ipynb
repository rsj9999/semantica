{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ciencias3.txt',\n",
       " 'economia6.txt',\n",
       " 'ciencias2.txt',\n",
       " 'economia3.txt',\n",
       " 'deportes4.txt',\n",
       " 'deportes1.txt',\n",
       " 'deportes3.txt',\n",
       " 'economia5.txt',\n",
       " 'ciencias5.txt',\n",
       " 'economia1.txt',\n",
       " 'economia4.txt',\n",
       " 'economia2.txt',\n",
       " 'deportes2.txt',\n",
       " 'ciencias6.txt',\n",
       " 'ciencias1.txt',\n",
       " 'ciencias4.txt',\n",
       " 'deportes5.txt',\n",
       " 'deportes6.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('noticias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_list = []\n",
    "for i, noti in enumerate(os.listdir('noticias')):\n",
    "    docs_list.append('')\n",
    "    with open(\"noticias/\"+noti) as f:\n",
    "        for line in f:\n",
    "            docs_list[i] = docs_list[i] + line.replace('\\n',' ')\n",
    "    f.close()\n",
    "len(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stop_list = []\n",
    "with open(\"stopwords.txt\") as f:\n",
    "    for line in f:\n",
    "        stop_list.append(line.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "https://towardsdatascience.com/topic-modeling-for-the-new-york-times-news-dataset-1f643e15caac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras que más aparecen: \n",
      "Palabra 5 (\"américa\") aparece 15 veces.\n",
      "Palabra 151 (\"argentina\") aparece 12 veces.\n",
      "Palabra 76 (\"ingresos\") aparece 11 veces.\n",
      "Palabra 159 (\"fútbol\") aparece 10 veces.\n",
      "Palabra 88 (\"personas\") aparece 10 veces.\n",
      "Palabra 32 (\"negros\") aparece 9 veces.\n",
      "Palabra 159 (\"fútbol\") aparece 9 veces.\n",
      "Palabra 148 (\"países\") aparece 8 veces.\n",
      "Palabra 85 (\"negocio\") aparece 8 veces.\n",
      "Palabra 137 (\"economía\") aparece 8 veces.\n",
      "Palabra 3 (\"agujeros\") aparece 7 veces.\n",
      "Palabra 163 (\"2020\") aparece 7 veces.\n",
      "Palabra 80 (\"mercado\") aparece 6 veces.\n",
      "Palabra 12 (\"contenido\") aparece 6 veces.\n",
      "Palabra 22 (\"gravedad\") aparece 6 veces.\n",
      "\n",
      "Algoritmo LDA usando un diccionario de palabras: \n",
      "Topic: 1 \n",
      "Words: 0.058*\"ingresos\" + 0.027*\"selección\" + 0.027*\"técnico\" + 0.027*\"argentina\"\n",
      "Topic: 2 \n",
      "Words: 0.066*\"fútbol\" + 0.047*\"américa\" + 0.041*\"argentina\" + 0.035*\"presidente\"\n",
      "Topic: 3 \n",
      "Words: 0.076*\"compañía\" + 0.049*\"mercado\" + 0.049*\"firma\" + 0.042*\"millones\"\n",
      "Topic: 4 \n",
      "Words: 0.078*\"latina\" + 0.062*\"negocio\" + 0.062*\"ingresos\" + 0.047*\"unidos\"\n",
      "Topic: 5 \n",
      "Words: 0.068*\"personas\" + 0.056*\"universidad\" + 0.043*\"investigación\" + 0.031*\"problemas\"\n",
      "Topic: 6 \n",
      "Words: 0.055*\"número\" + 0.055*\"publicación\" + 0.055*\"hospital\" + 0.028*\"universidad\"\n",
      "Topic: 7 \n",
      "Words: 0.035*\"negros\" + 0.034*\"millones\" + 0.034*\"científicos\" + 0.029*\"agujeros\"\n",
      "Topic: 8 \n",
      "Words: 0.056*\"economía\" + 0.056*\"negocio\" + 0.042*\"mercado\" + 0.035*\"rival\"\n",
      "\n",
      "Algoritmo LDA usando un objeto tf-idf: \n",
      "Topic: 1 \n",
      "Word: 0.004*\"argentino\" + 0.004*\"director\" + 0.004*\"social\" + 0.004*\"comunicado\"\n",
      "Topic: 2 \n",
      "Word: 0.004*\"argentino\" + 0.004*\"director\" + 0.004*\"social\" + 0.004*\"comunicado\"\n",
      "Topic: 3 \n",
      "Word: 0.028*\"agujeros\" + 0.028*\"negros\" + 0.026*\"científicos\" + 0.022*\"grande\"\n",
      "Topic: 4 \n",
      "Word: 0.026*\"ingresos\" + 0.026*\"publicación\" + 0.026*\"hospital\" + 0.018*\"medios\"\n",
      "Topic: 5 \n",
      "Word: 0.029*\"fútbol\" + 0.028*\"selección\" + 0.028*\"argentina\" + 0.022*\"personas\"\n",
      "Topic: 6 \n",
      "Word: 0.027*\"compañía\" + 0.026*\"negocio\" + 0.022*\"ingresos\" + 0.021*\"firma\"\n",
      "Topic: 7 \n",
      "Word: 0.030*\"mercado\" + 0.024*\"presidente\" + 0.019*\"gravedad\" + 0.018*\"obtener\"\n",
      "Topic: 8 \n",
      "Word: 0.028*\"argentino\" + 0.028*\"noche\" + 0.024*\"hora\" + 0.023*\"economía\"\n"
     ]
    }
   ],
   "source": [
    "doc_complete = docs_list\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "# from gensim.utils import simple_preprocess\n",
    "# from gensim.parsing.preprocessing import STOPWORDS\n",
    "# from gensim import parsing\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "#need to install this library with this comand ->\n",
    "#nltk.download()\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "#stem might not be an actual word\n",
    "stemmer = SpanishStemmer()\n",
    "\n",
    "#lemma is an actual language word\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "#stop word is a commonly used word (such as “the”, “in”) that a search engine has been programmed to ignore\n",
    "stop = set(stopwords.words('spanish'))\n",
    "stop = list(set().union(stop,stop_list))\n",
    "#puntuación a eliminar\n",
    "exclude = set().union(string.punctuation,'“”¿|/\\ºª@#~$€%½¬&{}[]()=`+*-¨´Çç_-:.;,¡!')\n",
    "\n",
    "#function to stem\n",
    "# def stemmer_stemming(text):\n",
    "#     return stemmer.stem(text)\n",
    "\n",
    "def clean(doc):\n",
    "#     print(doc)\n",
    "    #Quitamos las stop words y aplicamos metodo lower\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop and len(i) > 3])\n",
    "    #removemos la puntuancion\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word, 'v') for word in word_tokenize(punc_free))\n",
    "#     print(normalized)\n",
    "#     result = []\n",
    "#     for token in gensim.utils.simple_preprocess(normalized):\n",
    "#         if token not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "#             result.append(stemmer_stemming(token))\n",
    "#     print(result)\n",
    "    return normalized\n",
    "\n",
    "#limpiamos la lista de documentos\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "\n",
    "#Creamos un diccionario a partir de 'doc_clean' que contenga el número de veces que una palabra aparece\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "#Filtramos los tokens que aparecen en:\n",
    "#     menos de n documentos (número absoluto) o\n",
    "#     más de % documentos (fracción del tamaño total del corpus, no número absoluto).\n",
    "#     después de los dos pasos anteriores, guarde sólo los primeros N tokens más frecuentes.\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.7, keep_n=1000)\n",
    "\n",
    "# Para cada documento creamos un diccionario de palabras con las veces que aparecen\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "#################\n",
    "#check dictionary\n",
    "\n",
    "dic_array =[]\n",
    "for i in doc_term_matrix:\n",
    "    for k in i:\n",
    "        if k[1] > 5:\n",
    "            dic_array.append([k[0],dictionary[k[0]],k[1]])\n",
    "print('Palabras que más aparecen: ')\n",
    "\n",
    "def sortSecond(val): \n",
    "    return val[2]\n",
    "dic_array.sort(key = sortSecond, reverse = True)\n",
    "\n",
    "for i in range(len(dic_array)):\n",
    "    print(\"Palabra {} (\\\"{}\\\") aparece {} veces.\".format(dic_array[i][0], dic_array[i][1], dic_array[i][2]))\n",
    "print(\"\")\n",
    "#################\n",
    "Nro_topis = 8\n",
    "Nro_words = 4\n",
    "\n",
    "#algoritmo LDA usando un diccionario de palabras\n",
    "print(\"Algoritmo LDA usando un diccionario de palabras: \")\n",
    "Lda = gensim.models.LdaMulticore\n",
    "lda_model = Lda(doc_term_matrix, num_topics=Nro_topis, id2word = dictionary, passes=30, alpha=.001, eta=.05, workers=3)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(num_topics=Nro_topis, num_words=Nro_words):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx+1, topic))\n",
    "\n",
    "# lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "print(\"\")\n",
    "#creamos un objeto tf-idf \"Term frequency – Inverse document frequency\"\n",
    "tfidf = models.TfidfModel(doc_term_matrix, normalize=True)\n",
    "corpus_tfidf = tfidf[doc_term_matrix]\n",
    "\n",
    "#algoritmo LDA usando un objeto tf-idf \"Term frequency – Inverse document frequency\"\n",
    "print(\"Algoritmo LDA usando un objeto tf-idf: \")\n",
    "lda_model_tfidf = Lda(corpus_tfidf, num_topics=Nro_topis, id2word = dictionary, passes=30, alpha=.0005, eta=.05, workers=3)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(num_topics=Nro_topis, num_words=Nro_words):\n",
    "    print('Topic: {} \\nWord: {}'.format(idx+1, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x7fb63631b048>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probando en test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list_test = ['']\n",
    "with open(\"test_eco.txt\") as f:\n",
    "    for line in f:\n",
    "        docs_list_test[0] = docs_list_test[0] + line.replace('\\n',' ')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean_test = [clean(doc).split() for doc in docs_list_test]\n",
    "doc_term_matrix_test = [dictionary.doc2bow(doc) for doc in doc_clean_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5112519860267639\t Topic: 0.076*\"compañía\" + 0.049*\"mercado\" + 0.049*\"firma\" + 0.042*\"millones\"\n",
      "Score: 0.2332012802362442\t Topic: 0.035*\"negros\" + 0.034*\"millones\" + 0.034*\"científicos\" + 0.029*\"agujeros\"\n",
      "Score: 0.1384214162826538\t Topic: 0.066*\"fútbol\" + 0.047*\"américa\" + 0.041*\"argentina\" + 0.035*\"presidente\"\n",
      "Score: 0.06454111635684967\t Topic: 0.078*\"latina\" + 0.062*\"negocio\" + 0.062*\"ingresos\" + 0.047*\"unidos\"\n",
      "Score: 0.052524253726005554\t Topic: 0.058*\"ingresos\" + 0.027*\"selección\" + 0.027*\"técnico\" + 0.027*\"argentina\"\n"
     ]
    }
   ],
   "source": [
    "bow_vector = doc_term_matrix_test[0]\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda x: -1*x[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, Nro_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lda_model_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.6342630386352539\t \n",
      "Topic: 0.027*\"compañía\" + 0.026*\"negocio\" + 0.022*\"ingresos\" + 0.021*\"firma\"\n",
      "\n",
      "Score: 0.18710722029209137\t \n",
      "Topic: 0.028*\"agujeros\" + 0.028*\"negros\" + 0.026*\"científicos\" + 0.022*\"grande\"\n",
      "\n",
      "Score: 0.1136845201253891\t \n",
      "Topic: 0.029*\"fútbol\" + 0.028*\"selección\" + 0.028*\"argentina\" + 0.022*\"personas\"\n",
      "\n",
      "Score: 0.06490522623062134\t \n",
      "Topic: 0.030*\"mercado\" + 0.024*\"presidente\" + 0.019*\"gravedad\" + 0.018*\"obtener\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda x: -1*x[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, Nro_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list_test = ['']\n",
    "with open(\"test_dep.txt\") as f:\n",
    "    for line in f:\n",
    "        docs_list_test[0] = docs_list_test[0] + line.replace('\\n',' ')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean_test = [clean(doc).split() for doc in docs_list_test]\n",
    "doc_term_matrix_test = [dictionary.doc2bow(doc) for doc in doc_clean_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4928743243217468\t Topic: 0.066*\"fútbol\" + 0.047*\"américa\" + 0.041*\"argentina\" + 0.035*\"presidente\"\n",
      "Score: 0.20012427866458893\t Topic: 0.058*\"ingresos\" + 0.027*\"selección\" + 0.027*\"técnico\" + 0.027*\"argentina\"\n",
      "Score: 0.1964830905199051\t Topic: 0.035*\"negros\" + 0.034*\"millones\" + 0.034*\"científicos\" + 0.029*\"agujeros\"\n",
      "Score: 0.11039335280656815\t Topic: 0.055*\"número\" + 0.055*\"publicación\" + 0.055*\"hospital\" + 0.028*\"universidad\"\n"
     ]
    }
   ],
   "source": [
    "bow_vector = doc_term_matrix_test[0]\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda x: -1*x[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, Nro_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lda_model_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.4107641279697418\t \n",
      "Topic: 0.029*\"fútbol\" + 0.028*\"selección\" + 0.028*\"argentina\" + 0.022*\"personas\"\n",
      "\n",
      "Score: 0.24102327227592468\t \n",
      "Topic: 0.027*\"compañía\" + 0.026*\"negocio\" + 0.022*\"ingresos\" + 0.021*\"firma\"\n",
      "\n",
      "Score: 0.18949885666370392\t \n",
      "Topic: 0.026*\"ingresos\" + 0.026*\"publicación\" + 0.026*\"hospital\" + 0.018*\"medios\"\n",
      "\n",
      "Score: 0.1072831004858017\t \n",
      "Topic: 0.028*\"agujeros\" + 0.028*\"negros\" + 0.026*\"científicos\" + 0.022*\"grande\"\n",
      "\n",
      "Score: 0.05138380452990532\t \n",
      "Topic: 0.030*\"mercado\" + 0.024*\"presidente\" + 0.019*\"gravedad\" + 0.018*\"obtener\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda x: -1*x[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, Nro_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ciencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list_test = ['']\n",
    "with open(\"test_cie.txt\") as f:\n",
    "    for line in f:\n",
    "        docs_list_test[0] = docs_list_test[0] + line.replace('\\n',' ')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean_test = [clean(doc).split() for doc in docs_list_test]\n",
    "doc_term_matrix_test = [dictionary.doc2bow(doc) for doc in doc_clean_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7739781141281128\t Topic: 0.035*\"negros\" + 0.034*\"millones\" + 0.034*\"científicos\" + 0.029*\"agujeros\"\n",
      "Score: 0.1496550738811493\t Topic: 0.068*\"personas\" + 0.056*\"universidad\" + 0.043*\"investigación\" + 0.031*\"problemas\"\n",
      "Score: 0.06052404269576073\t Topic: 0.056*\"economía\" + 0.056*\"negocio\" + 0.042*\"mercado\" + 0.035*\"rival\"\n",
      "Score: 0.015815572813153267\t Topic: 0.066*\"fútbol\" + 0.047*\"américa\" + 0.041*\"argentina\" + 0.035*\"presidente\"\n"
     ]
    }
   ],
   "source": [
    "bow_vector = doc_term_matrix_test[0]\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda x: -1*x[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, Nro_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lda_model_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5811505913734436\t \n",
      "Topic: 0.028*\"agujeros\" + 0.028*\"negros\" + 0.026*\"científicos\" + 0.022*\"grande\"\n",
      "\n",
      "Score: 0.17511488497257233\t \n",
      "Topic: 0.029*\"fútbol\" + 0.028*\"selección\" + 0.028*\"argentina\" + 0.022*\"personas\"\n",
      "\n",
      "Score: 0.13132624328136444\t \n",
      "Topic: 0.030*\"mercado\" + 0.024*\"presidente\" + 0.019*\"gravedad\" + 0.018*\"obtener\"\n",
      "\n",
      "Score: 0.05026784539222717\t \n",
      "Topic: 0.028*\"argentino\" + 0.028*\"noche\" + 0.024*\"hora\" + 0.023*\"economía\"\n",
      "\n",
      "Score: 0.03475542739033699\t \n",
      "Topic: 0.027*\"compañía\" + 0.026*\"negocio\" + 0.022*\"ingresos\" + 0.021*\"firma\"\n",
      "\n",
      "Score: 0.027378197759389877\t \n",
      "Topic: 0.026*\"ingresos\" + 0.026*\"publicación\" + 0.026*\"hospital\" + 0.018*\"medios\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda x: -1*x[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, Nro_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train:\n",
    "- deportes1 https://www.bbc.com/mundo/deportes-47874344\n",
    "- deportes2 https://www.bbc.com/mundo/deportes-47867626\n",
    "- deportes3 https://www.bbc.com/mundo/deportes-47867623\n",
    "- deportes4 https://www.bbc.com/mundo/deportes-47867621\n",
    "- deportes5 https://www.bbc.com/mundo/deportes-47827846\n",
    "- deportes6 https://www.bbc.com/mundo/noticias-47758928\n",
    "- ciencias1 https://www.bbc.com/mundo/noticias-47915625\n",
    "- ciencias2 https://www.bbc.com/mundo/noticias-47859720\n",
    "- ciencias3 https://www.bbc.com/mundo/noticias-47915620\n",
    "- ciencias4 https://www.bbc.com/mundo/noticias-47909000\n",
    "- ciencias5 https://www.bbc.com/mundo/noticias-47860116\n",
    "- ciencias6 https://www.bbc.com/mundo/noticias-47900442\n",
    "- economia1 https://www.bbc.com/mundo/noticias-47861934\n",
    "- economia2 https://www.bbc.com/mundo/noticias-47670796\n",
    "- economia3 https://www.bbc.com/mundo/noticias-47901139\n",
    "- economia4 https://www.bbc.com/mundo/noticias-47908922\n",
    "- economia5 https://www.bbc.com/mundo/noticias-47900832\n",
    "- economia6 https://www.bbc.com/mundo/noticias-47895086\n",
    "\n",
    "\n",
    "Test:\n",
    "- test_cie https://www.bbc.com/mundo/noticias-47884630\n",
    "- test dep https://www.bbc.com/mundo/deportes-47667089\n",
    "- test_eco https://www.bbc.com/mundo/noticias-47872006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
